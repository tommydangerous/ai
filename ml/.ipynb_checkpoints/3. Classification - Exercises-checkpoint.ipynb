{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_csv(path, file_name):\n",
    "    csv_path = os.path.join(path, file_name)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def plot_number(digit):\n",
    "    image = digit.reshape(28, 28)\n",
    "    plt.imshow(\n",
    "        image,\n",
    "        cmap=matplotlib.cm.binary,\n",
    "        interpolation='nearest',\n",
    "    )\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def triple_digit_number(n):\n",
    "    if n < 10:\n",
    "        return '00{}'.format(n)\n",
    "    elif n < 100:\n",
    "        return '0{}'.format(n)\n",
    "    else:\n",
    "        return str(n)\n",
    "\n",
    "def print_image(image, dimension=None):\n",
    "    if dimension:\n",
    "        width, height = dimension\n",
    "    else:\n",
    "        width = int(sqrt(len(image)))\n",
    "        height = width\n",
    "    for h in range(0, height):\n",
    "        row = image[(h * width):((h + 1) * width)]\n",
    "        print(''.join([triple_digit_number(n) for n in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def empty_pixels(n):\n",
    "    return [0 for i in range(0, n)]\n",
    "\n",
    "def shift_image_horizontal(image, n_pixels, opts={}):\n",
    "    dimension = opts.get('dimension', None)\n",
    "    direction = opts.get('direction', 'left')\n",
    "    \n",
    "    if dimension:\n",
    "        width, height = dimension\n",
    "    else:\n",
    "        width = int(sqrt(len(image)))\n",
    "        height = width\n",
    "    new_image = image.copy()\n",
    "    \n",
    "    for h in range(0, height):\n",
    "        diff = width - n_pixels\n",
    "        \n",
    "        cut_idx_start = h * width\n",
    "        if direction == 'left':\n",
    "            cut_idx_start += n_pixels\n",
    "        cut_idx_end = cut_idx_start + diff\n",
    "        \n",
    "        paste_idx_start = h * width\n",
    "        if direction != 'left':\n",
    "            paste_idx_start += n_pixels\n",
    "        paste_idx_end = paste_idx_start + diff\n",
    "        \n",
    "        if direction == 'left':\n",
    "            fill_idx_start = paste_idx_end\n",
    "            fill_idx_end = paste_idx_end + n_pixels\n",
    "        else:\n",
    "            fill_idx_start = paste_idx_start - n_pixels\n",
    "            fill_idx_end = paste_idx_start\n",
    "        \n",
    "        new_image[paste_idx_start:paste_idx_end] = new_image[cut_idx_start:cut_idx_end]\n",
    "        new_image[fill_idx_start:fill_idx_end] = empty_pixels(n_pixels)\n",
    "    return new_image\n",
    "\n",
    "def shift_image_vertical(image, n_pixels, opts={}):\n",
    "    dimension = opts.get('dimension', None)\n",
    "    direction = opts.get('direction', 'down')\n",
    "    \n",
    "    if dimension:\n",
    "        width, height = dimension\n",
    "    else:\n",
    "        width = int(sqrt(len(image)))\n",
    "        height = width\n",
    "    new_image = image.copy()\n",
    "    \n",
    "    size = (height - n_pixels) * width\n",
    "    \n",
    "    if direction == 'down':\n",
    "        cut_idx_start = 0\n",
    "    else:\n",
    "        cut_idx_start = n_pixels * width\n",
    "    cut_idx_end = cut_idx_start + size\n",
    "    \n",
    "    if direction == 'down':\n",
    "        paste_idx_start = n_pixels * width\n",
    "    else:\n",
    "        paste_idx_start = 0\n",
    "    paste_idx_end = paste_idx_start + size\n",
    "    \n",
    "    if direction == 'down':\n",
    "        fill_idx_start = cut_idx_start\n",
    "    else:\n",
    "        fill_idx_start = paste_idx_end\n",
    "    fill_idx_end = fill_idx_start + (n_pixels * width)\n",
    "    \n",
    "    new_image[paste_idx_start:paste_idx_end] = new_image[cut_idx_start:cut_idx_end]\n",
    "    new_image[fill_idx_start:fill_idx_end] = empty_pixels(fill_idx_end - fill_idx_start)\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "def shift_image(image, n_pixels, opts={}):\n",
    "    direction = opts.get('direction', 'left')\n",
    "    if direction in ['left', 'right']:\n",
    "        return shift_image_horizontal(image, n_pixels, opts)\n",
    "    else:\n",
    "        return shift_image_vertical(image, n_pixels, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def stratify(data, column, opts={}):\n",
    "    stratified_split = StratifiedShuffleSplit(\n",
    "        n_splits=opts.get('n_splits', 1),\n",
    "        test_size=opts.get('test_size', 0.2),\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    gen = stratified_split.split(\n",
    "        data,\n",
    "        data[column],\n",
    "    )\n",
    "    \n",
    "    training_sets = []\n",
    "    test_sets = []\n",
    "\n",
    "    for training_indices, test_indices in gen:\n",
    "        training_sets.append(data.iloc[training_indices])\n",
    "        test_sets.append(data.iloc[test_indices])\n",
    "        \n",
    "    return training_sets, test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = mnist['data'], mnist['target']\n",
    "training_idx = 60000\n",
    "X_train, X_test = X[:training_idx], X[training_idx:]\n",
    "y_train, y_test = y[:training_idx], y[training_idx:]\n",
    "\n",
    "shuffle_index_train = np.random.permutation(len(X_train))\n",
    "shuffle_index_test = np.random.permutation(len(X_test))\n",
    "\n",
    "X_train, y_train = X_train[shuffle_index_train], y_train[shuffle_index_train]\n",
    "X_test, y_test = X_test[shuffle_index_test], y_test[shuffle_index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\n",
    "\n",
    "Try to build a classifier for the MNIST dataset that achieves over\n",
    "97% accuracy on the test set.\n",
    "\n",
    "Hint: the `KNeighborsClassifier` works quite\n",
    "well for this task; you just need to find good hyperparameter values\n",
    "(try a grid search on the weights and n_neighbors hyperparameters).\n",
    "\n",
    "## Results\n",
    "- Random: 10% accuracy\n",
    "- 5 n_neighbors, uniform weights: 96.88% accuracy\n",
    "- 7, uniform: 96.94%\n",
    "- 5, distance: 96.90999% accuracy\n",
    "- 7, distance: 96.999999999999997%\n",
    "- 9, distance: 96.730000000000005%\n",
    "- **KNN 8, distance: 97.060000000000002%** (Winner)\n",
    "\n",
    "### Using Augmented Data\n",
    "- KNN 8 n_neighbors, distance weights = 97.28%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=8, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'n_neighbors': [10],\n",
    "        'weights': ['distance'],\n",
    "    },\n",
    "]\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_grid_search = GridSearchCV(\n",
    "    knn_model,\n",
    "    param_grid,\n",
    "    cv=2,\n",
    "    scoring='accuracy',\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_grid_search.best_params_\n",
    "knn_grid_search.best_estimator_\n",
    "knn_grid_search.cv_results_\n",
    "best_model = knn_grid_search.best_estimator_\n",
    "best_predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(best_predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "\n",
    "Write a function that can shift an MNIST image in any direction\n",
    "(left, right, up, or down) by one pixel.\n",
    "\n",
    "Then, for each image in the training set, create four shifted\n",
    "copies (one per direction) and add them to the training set.\n",
    "\n",
    "Finally, train your best model on this expanded training set and\n",
    "measure its accuracy on the test set.\n",
    "\n",
    "You should observe that your model performs even better now!\n",
    "This technique of artificially growing the training set is called\n",
    "**data augmentation** or **training set expansion**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_augmented = []\n",
    "directions = ['down', 'left', 'right', 'up']\n",
    "\n",
    "for image in X_train:\n",
    "    direction = random.choice(directions)\n",
    "    new_image = shift_image(image, 1, { 'direction': direction })\n",
    "    image_array = np.array([new_image])\n",
    "    if len(X_train_augmented) == 0:\n",
    "        X_train_augmented = image_array\n",
    "    else:\n",
    "        X_train_augmented = np.append(X_train_augmented, image_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_combined = np.concatenate((X_train, X_train_augmented))\n",
    "y_train_combined = np.concatenate((y_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train_combined, y_train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "Tackle the *Titanic* dataset.\n",
    "A great place to start is on\n",
    "[Kaggle](https://www.kaggle.com/c/titanic).\n",
    "\n",
    "[Feature engineering](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/).\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her\n",
    "maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n",
    "This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the\n",
    "passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of\n",
    "people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In\n",
    "particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITANIC_PATH = 'datasets/titanic'\n",
    "gender_submission = read_csv(TITANIC_PATH, 'gender_submission.csv')\n",
    "train_original = read_csv(TITANIC_PATH, 'train.csv')\n",
    "test_original = read_csv(TITANIC_PATH, 'test.csv')\n",
    "train = train_original.copy()\n",
    "test = test_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Survived'] = gender_submission['Survived']\n",
    "combined = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(x):\n",
    "    return re.findall('\\w+\\.', x)[0]\n",
    "\n",
    "combined['Title'] = list(map(lambda x: extract_title(x), combined['Name']))\n",
    "all_titles = sorted(list(set(combined['Title'])))\n",
    "\n",
    "# train['Title'] = list(map(lambda x: extract_title(x), train['Name']))\n",
    "# test['Title'] = list(map(lambda x: extract_title(x), test['Name']))\n",
    "\n",
    "# train_title_set = set(train['Title'])\n",
    "# test_title_set = set(test['Title'])\n",
    "# missing_titles = [x for x in train_title_set if x not in test_title_set]\n",
    "# all_titles = list(train_title_set.union(test_title_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_strings(x):\n",
    "    return re.findall('[A-Z]+', x)\n",
    "    \n",
    "# cabin_values = list(set(train['Cabin'][train['Cabin'].notnull()].values))\n",
    "# cabin_values_letters = [extract_strings(x) for x in cabin_values]\n",
    "# cabin_letters = sorted(list(set([item for sublist in cabin_values_letters for item in sublist])))\n",
    "\n",
    "import math\n",
    "\n",
    "def deck_letter(x):\n",
    "    if type(x) != str and math.isnan(x):\n",
    "        return ''.join(cabin_letters)\n",
    "    letters = extract_strings(x)\n",
    "    most_common, num_most_common = Counter(letters).most_common(1)[0]\n",
    "    return most_common\n",
    "\n",
    "combined['Deck'] = list(map(lambda x: deck_letter(x), combined['Cabin']))\n",
    "all_decks = sorted(list(set(combined['Deck'])))\n",
    "\n",
    "# train['Deck'] = list(map(lambda x: deck_letter(x), train['Cabin']))\n",
    "# test['Deck'] = list(map(lambda x: deck_letter(x), test['Cabin']))\n",
    "\n",
    "# train_deck_set = set(train['Deck'])\n",
    "# test_deck_set = set(test['Deck'])\n",
    "# missing_decks = [x for x in train_deck_set if x not in test_deck_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "gen = stratified_split.split(\n",
    "    combined,\n",
    "    combined['Sex'],\n",
    ")\n",
    "\n",
    "for tr, te in gen:\n",
    "    training_indices = tr\n",
    "    test_indices = te\n",
    "    training_set = combined.iloc[training_indices]\n",
    "    test_set = combined.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer, StandardScaler\n",
    "\n",
    "class DateFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "class CabinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        copy = X['Cabin'].copy()\n",
    "        copy = copy.fillna('Z')\n",
    "        return copy.apply(self.transform_cabin).values\n",
    "    \n",
    "    def transform_cabin(self, x):\n",
    "        return ''.join(re.findall('[A-Z]+', x))\n",
    "    \n",
    "class CategoryEmptyFiller(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        empty = 'EMPTY'\n",
    "        copy = X.copy()\n",
    "        copy[self.attribute_names] = copy[self.attribute_names].fillna(empty)\n",
    "        return copy.append(\n",
    "            pd.DataFrame([['EMPTY' for i in self.attribute_names]], columns=self.attribute_names)\n",
    "        )\n",
    "\n",
    "class CategoryMissingTypeFiller(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_name, missing_types):\n",
    "        self.attribute_name = attribute_name\n",
    "        self.missing_types = missing_types\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        copy = X.copy()\n",
    "        return copy.append(\n",
    "            pd.DataFrame(\n",
    "                [[t] for t in self.missing_types],\n",
    "                columns=[self.attribute_name],\n",
    "            )\n",
    "        )\n",
    "    \n",
    "class CategoryEmptyFillerComplete(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[:-1]\n",
    "    \n",
    "class CategoryMissingTypeFillerComplete(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_types):\n",
    "        self.missing_types = missing_types\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        last_index = X.shape[0] - len(self.missing_types)\n",
    "        return X[:last_index]\n",
    "\n",
    "def cabin_pipeline():\n",
    "    return Pipeline([\n",
    "        ('cabin_transformer', CabinTransformer()),\n",
    "        ('label_binarizer', LabelBinarizer(sparse_output=True)),\n",
    "    ])\n",
    "\n",
    "def category_pipeline(attribute):\n",
    "    return Pipeline([\n",
    "        ('selector', DateFrameSelector(attribute)),\n",
    "        ('label_binarizer', LabelBinarizer(sparse_output=True)),\n",
    "    ])\n",
    "\n",
    "def category_pipeline_with_empty(attribute):\n",
    "    return Pipeline([\n",
    "        ('filler', CategoryEmptyFiller(attribute)),\n",
    "        ('selector', DateFrameSelector(attribute)),\n",
    "        ('label_binarizer', LabelBinarizer(sparse_output=True)),\n",
    "        ('filler_complete', CategoryEmptyFillerComplete()),\n",
    "    ])\n",
    "\n",
    "def category_pipeline_with_missing_types(attribute, missing_types):\n",
    "    return Pipeline([\n",
    "        ('filler', CategoryMissingTypeFiller(attribute, missing_types)),\n",
    "        ('selector', DateFrameSelector(attribute)),\n",
    "        ('label_binarizer', LabelBinarizer(sparse_output=True)),\n",
    "        ('filler_complete', CategoryMissingTypeFillerComplete(missing_types)),\n",
    "    ])\n",
    "\n",
    "def numerical_pipeline(attributes, strategy='mean'):\n",
    "    return Pipeline([\n",
    "        ('selector', DateFrameSelector(attributes)),\n",
    "        ('mean', Imputer(strategy=strategy)),\n",
    "        ('standard_scalar', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "def encoder_pipelines(attributes=[]):\n",
    "    return [('cat_pipe_{}'.format(attr), category_pipeline(attr)) for attr in attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name, Ticket (need text transformation)\n",
    "# Deck\n",
    "# Family size\n",
    "# Age class\n",
    "# Fare per person\n",
    "\n",
    "categorical_attributes = ['Sex']\n",
    "numerical_attributes = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "transformer_list = [\n",
    "#     ('cabin_pipeline', cabin_pipeline()),\n",
    "    ('numerical_attributes', numerical_pipeline(numerical_attributes)),\n",
    "    ('embarked', category_pipeline_with_empty(['Embarked'])),\n",
    "    ('missing_types_title', category_pipeline_with_missing_types('Title', all_titles)),\n",
    "    ('missing_types_deck', category_pipeline_with_missing_types('Deck', missing_decks)),\n",
    "] + encoder_pipelines(categorical_attributes)\n",
    "fp = FeatureUnion(transformer_list=transformer_list)\n",
    "train_prepared = fp.fit_transform(training_set)\n",
    "\n",
    "# category_pipeline_with_missing_types('Title', all_titles).fit_transform(train)\n",
    "# category_pipeline_with_missing_types('Title', all_titles).fit_transform(test)\n",
    "# LabelBinarizer(sparse_output=True).fit_transform(CategoryMissingTypeFiller('Title', missing_titles).fit_transform(test)['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 1090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmark\n",
    "# Guessing everyone died: 61.616% accuracy\n",
    "# Guessing everyone survived: 38.3838% accuracy\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# train_labels = train['Survived']\n",
    "train_labels = training_set['Survived']\n",
    "# print(list(train_labels == 0).count(True) / len(train_labels))\n",
    "# print(list(train_labels == 1).count(True) / len(train_labels))\n",
    "\n",
    "sgdc = SGDClassifier(random_state=42)\n",
    "sgdc.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047 37\n",
      "262 37\n",
      "0.782442748092\n"
     ]
    }
   ],
   "source": [
    "# SGD Classifier: 77.51%\n",
    "# SGD Classifier: 85.17% w/ Deck\n",
    "# SGD Classifier: 71.05% w/ Title\n",
    "# --\n",
    "# Stratify\n",
    "# 83.21% w/ sex alone\n",
    "# 80.15% w/ numerical attributes\n",
    "# 77.48% w/ embarked\n",
    "# 80.91% w/ title\n",
    "\n",
    "test_prepared = fp.fit_transform(test_set)\n",
    "# test_labels = gender_submission['Survived']\n",
    "test_labels = test_set['Survived']\n",
    "\n",
    "predictions = sgdc.predict(test_prepared)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(train_prepared.shape[0], train_prepared.shape[1])\n",
    "print(test_prepared.shape[0], test_prepared.shape[1])\n",
    "print(accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\n",
    "\n",
    "Build a spam classifier (a more challenging exercise):\n",
    "\n",
    "- Download examples of spam and ham from\n",
    "[public datasets](https://spamassassin.apache.org/publiccorpus/)\n",
    "\n",
    "- Unzip the datasets and familiarize yourself with the data format\n",
    "\n",
    "- Split the datasets into a training set and a test set\n",
    "\n",
    "- Write a data preparation pipeline to convert each email into a\n",
    "feature vector. Your preparation pipeline should transform an email\n",
    "into a (sparse) vector indicating the presence or absence of each\n",
    "possible word. For example, if all emails only ever contain four\n",
    "words, “Hello,” “how,” “are,” “you,” then the email “Hello you Hello\n",
    "Hello you” would be converted into a vector [1, 0, 0, 1] (meaning\n",
    "[“Hello” is present, “how” is absent, “are” is absent, “you” is\n",
    "present]), or [3, 0, 0, 2] if you prefer to count the number of\n",
    "occurrences of each word.\n",
    "\n",
    "- You may want to add hyperparameters to your preparation pipeline to\n",
    "control whether or not to strip off email headers, convert each email\n",
    "to lowercase, remove punctuation, replace all URLs with “URL,” replace\n",
    "all numbers with “NUMBER,” or even perform stemming (i.e., trim off\n",
    "word endings; there are Python libraries available to do this).\n",
    "\n",
    "- Then try out several classifiers and see if you can build a great\n",
    "spam classifier, with both high recall and high precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "\n",
    "def split_punctuation(word):\n",
    "    arr = re.split('\\W|\\d', word)\n",
    "    return list(filter(lambda x: len(x) >= 1, arr))\n",
    "\n",
    "def split_line(line):\n",
    "    arr = line.split('\\t')\n",
    "    if len(arr) != 2:\n",
    "        return arr\n",
    "    text = arr[1]\n",
    "    \n",
    "    [all_words.add(w.lower()) for w in split_punctuation(text)]\n",
    "    return [text, arr[0]]\n",
    "\n",
    "def flatten(arr):\n",
    "    return [item for sublist in arr for item in sublist]\n",
    "\n",
    "def read_table(path, file_name):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    return pd.read_table(file_path, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data_file = open('datasets/smsspam/SMSSpamCollection', 'r')\n",
    "text = data_file.read()\n",
    "lines = text.split('\\n')\n",
    "data = pd.DataFrame(data=[split_line(line) for line in lines], columns=['Text', 'Label'])[:-1]\n",
    "\n",
    "stop_words_file = open('datasets/smsspam/terrier-stop.txt', 'r')\n",
    "stop_words = stop_words_file.read().split('\\n')\n",
    "stop_words_dict = { word: True for word in stop_words }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7464\n"
     ]
    }
   ],
   "source": [
    "labels = set(data['Label'])\n",
    "\n",
    "def valid_word_to_use(word):\n",
    "    return word not in stop_words_dict and word not in labels\n",
    "\n",
    "def clean_line(line):\n",
    "    return [x.lower() for x in split_punctuation(line) if valid_word_to_use(x.lower())]\n",
    "\n",
    "# 0. Create word dictionary\n",
    "word_dictionary = set(sorted(\n",
    "    [x.lower() for x in set(split_punctuation(text)) if valid_word_to_use(x.lower())]\n",
    "))\n",
    "print(len(word_dictionary))\n",
    "\n",
    "# Pipeline\n",
    "# 1. Lowercase\n",
    "# 2. Remove punctuation\n",
    "# 3. Remove stop words\n",
    "# 4. Create feature vector for all words in word dictionary with count\n",
    "\n",
    "class LineToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, word_list, total_word_count=True):\n",
    "        self.total_word_count = total_word_count\n",
    "        self.word_list = word_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        copy = X.copy()\n",
    "        rows = list(map(lambda x: self.transform_row(x), copy['Text'].values))\n",
    "        return rows\n",
    "    \n",
    "    def transform_row(self, line):\n",
    "        clean = clean_line(line)\n",
    "        line_dict = { word: clean.count(word) for word in clean }\n",
    "        vector = [self.check_word(word, line_dict) for word in self.word_list]\n",
    "        return vector\n",
    "    \n",
    "    def check_word(self, word, mapping):\n",
    "        count = mapping.get(word, 0)\n",
    "        if self.total_word_count:\n",
    "            return count\n",
    "        elif count >= 1:\n",
    "            return 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4459 1115\n"
     ]
    }
   ],
   "source": [
    "pipeline = LineToVectorTransformer(word_dictionary, total_word_count=False)\n",
    "\n",
    "training_sets, test_sets = stratify(data, 'Label')\n",
    "train = training_sets[0]\n",
    "test = test_sets[0]\n",
    "print(len(train), len(test))\n",
    "train_prepared = pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 1387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train['Label']\n",
    "sgdc = SGDClassifier(random_state=42)\n",
    "sgdc.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1388,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepared = pipeline.fit_transform(test)\n",
    "predictions = sgdc.predict(test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "Precision: 0.951048951048951, Recall: 0.912751677852349\n",
      "ham\n",
      "Precision: 0.9866255144032922, Recall: 0.9927536231884058\n"
     ]
    }
   ],
   "source": [
    "# Total word count\n",
    "# spam\n",
    "# Precision: 0.9507042253521126, Recall: 0.9060402684563759\n",
    "# ham\n",
    "# Precision: 0.9856115107913669, Recall: 0.9927536231884058\n",
    "\n",
    "# Only count once for each word existence\n",
    "# spam\n",
    "# Precision: 0.951048951048951, Recall: 0.912751677852349\n",
    "# ham\n",
    "# Precision: 0.9866255144032922, Recall: 0.9927536231884058\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "label_list = list(labels)\n",
    "\n",
    "precision_scores = precision_score(test['Label'], predictions, average=None, labels=label_list)\n",
    "recall_scores = recall_score(test['Label'], predictions, average=None, labels=label_list)\n",
    "\n",
    "for index, label in enumerate(list(labels)):\n",
    "    print(label)\n",
    "    print('Precision: {}, Recall: {}'.format(precision_scores[index], recall_scores[index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
