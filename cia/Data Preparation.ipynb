{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_by(df, columns):\n",
    "    group = df.groupby(columns, axis=0)\n",
    "    return [(key, group.get_group(key)) for key in group.groups.keys()]\n",
    "\n",
    "def group_by_hour(df):\n",
    "    return sorted(group_by(df, ['hour']), key=lambda x: x[0])\n",
    "    \n",
    "def group_by_date(df):\n",
    "    return sorted(group_by(df, ['date']), key=lambda x: x[0])\n",
    "\n",
    "def group_by_currency(df):\n",
    "    return sorted(group_by(df, ['currency']), key=lambda x: x[0])\n",
    "\n",
    "def transform_data_for_hour(df):\n",
    "    group_sorted = df.sort_values(\n",
    "        ['timestamp_close'], ascending=[1]\n",
    "    ).drop_duplicates(\n",
    "        'timestamp_close', keep='last'\n",
    "    )\n",
    "    volume = sum(group_sorted['quantity'])\n",
    "    price_open = group_sorted.iloc[0]['price_open']\n",
    "    closing_prices = group_sorted['price_close']\n",
    "    price_close = closing_prices.iloc[len(group_sorted) - 1]\n",
    "    price_high = max(closing_prices)\n",
    "    price_low = min(closing_prices)\n",
    "    return volume, price_open, price_close, price_high, price_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_columns = [\n",
    "    'date',\n",
    "    'hour',\n",
    "    'currency',\n",
    "    'volume',\n",
    "    'price_open',\n",
    "    'price_close',\n",
    "    'price_high',\n",
    "    'price_low',\n",
    "]\n",
    "\n",
    "def transform_all(df):\n",
    "    d = {}\n",
    "    for currency, g_by_c in group_by_currency(df):\n",
    "        arr = []\n",
    "        for date, g_by_d in group_by_date(g_by_c):\n",
    "            for hour, g_by_h in group_by_hour(g_by_d):\n",
    "                values = transform_data_for_hour(g_by_h)\n",
    "                arr.append((date, hour, currency) + values)\n",
    "        d[currency] = pd.DataFrame(data=arr, columns=new_columns)\n",
    "    return d\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data('datasets/BittrexChart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns = [\n",
    "    'created_at',\n",
    "    'currency',\n",
    "    'exchange',\n",
    "    'price',\n",
    "    'price_close',\n",
    "    'price_high',\n",
    "    'price_low',\n",
    "    'price_open',\n",
    "    'quantity',\n",
    "    'timestamp',\n",
    "    'timestamp_close',\n",
    "    'timestamp_open',\n",
    "    'uuid',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_and_hour(x):\n",
    "    date = datetime.datetime.fromtimestamp(x)\n",
    "    return int(date.strftime('%Y%m%d')), int(date.strftime('%H'))\n",
    "\n",
    "data['date'], data['hour'] = zip(*data['timestamp_close'].apply(date_and_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "yearsFmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "def date_and_hour(row):\n",
    "    date = row['date']\n",
    "    hour = row['hour']\n",
    "    return datetime.datetime.strptime(str(date), '%Y%m%d') + datetime.timedelta(hours=hour)\n",
    "    \n",
    "def plot_currency(currency, d1):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d'))\n",
    "    plt.gca().xaxis.set_minor_locator(mdates.HourLocator())\n",
    "    \n",
    "    datemin = datetime.datetime.strptime(str(min(d1['date'])), \"%Y%m%d\").date() - datetime.timedelta(days=1)\n",
    "    datemax = datetime.datetime.strptime(str(max(d1['date'])), \"%Y%m%d\").date() + datetime.timedelta(days=1)\n",
    "\n",
    "    plt.axis([\n",
    "        datemin,\n",
    "        datemax,\n",
    "        min(d1['price_close']),\n",
    "        max(d1['price_close']),\n",
    "    ])\n",
    "    \n",
    "    plt.plot(\n",
    "        d1.apply(date_and_hour, axis=1),\n",
    "        d1['price_close'],\n",
    "        'b.'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(currency)\n",
    "    \n",
    "# plot_currency('ZCL', df_transformed['ZCL'])\n",
    "# plot_currency('LTC', df_transformed['LTC'])\n",
    "# plot_currency('ETH', df_transformed['ETH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_transformed_initial = transform_all(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for currency, df in df_transformed_initial.items():\n",
    "    df.to_csv('data/currencies/{}.csv'.format(currency), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df_transformed = {}\n",
    "currencies = []\n",
    "for name in os.listdir('data/currencies'):\n",
    "    if not name.startswith('.') :\n",
    "        currencies.append(name.split('.csv')[0])\n",
    "\n",
    "for currency in currencies:\n",
    "    df_transformed[currency] = load_data('data/currencies/{}.csv'.format(currency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "\n",
    "columns_as_feature = [\n",
    "    'volume',\n",
    "    'price_open',\n",
    "    'price_close',\n",
    "    'price_high',\n",
    "    'price_low',\n",
    "]\n",
    "\n",
    "class DateFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "def build_pipeline(numerical_attributes):\n",
    "    scalar_pipeline = Pipeline([\n",
    "        ('selector', DateFrameSelector(numerical_attributes)),\n",
    "        ('standard_scalar', StandardScaler()),\n",
    "    ])\n",
    "    return FeatureUnion(transformer_list=[\n",
    "        ('scalar_pipeline', scalar_pipeline),\n",
    "    ])\n",
    "\n",
    "def scale_data(df):\n",
    "    pipeline = build_pipeline(columns_as_feature)\n",
    "    df_scaled = pd.DataFrame(data=pipeline.fit_transform(df), columns=columns_as_feature)\n",
    "    df_scaled[['date', 'hour_of_day']] = df[['date', 'hour']]\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hours = np.ndarray(shape=(24, 1), dtype=np.float64)\n",
    "for i in range(0, 24):\n",
    "    hours[i] = np.array([i], dtype=np.float64)\n",
    "hours_scaled = StandardScaler().fit_transform(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_data = {}\n",
    "original_data = {}\n",
    "\n",
    "for currency, df in df_transformed.items():\n",
    "    scaled_data[currency] = {}\n",
    "    original_data[currency] = {}\n",
    "    \n",
    "    for idx, row in scale_data(df).iterrows():\n",
    "        date = row['date']\n",
    "        hour_of_day = row['hour_of_day']\n",
    "        \n",
    "        if not scaled_data[currency].get(date, False):\n",
    "            scaled_data[currency][date] = {}\n",
    "            \n",
    "        scaled_data[currency][date][hour_of_day] = row[columns_as_feature]\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        hour_of_day = row['hour']\n",
    "            \n",
    "        if not original_data[currency].get(date, False):\n",
    "            original_data[currency][date] = {}\n",
    "            \n",
    "        original_data[currency][date][hour_of_day] = row[columns_as_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currencies = sorted(list(scaled_data.keys()))\n",
    "all_hours = sorted(flatten(hours))\n",
    "all_dates = sorted(list(set(\n",
    "    flatten([scaled_data[curr].keys() for curr in currencies])\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_initial_columns = [\n",
    "    'date',\n",
    "    'hour',\n",
    "    'hour_scaled',\n",
    "]\n",
    "currency_feature_columns = flatten(\n",
    "    [['{}_{}'.format(col_name, curr)for curr in currencies for col_name in columns_as_feature]]\n",
    ")\n",
    "df_shared_columns = shared_initial_columns + currency_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = []\n",
    "vectors_original = []\n",
    "\n",
    "for date in all_dates:\n",
    "    for hour in all_hours:\n",
    "        vector = [\n",
    "            date,\n",
    "            hour,\n",
    "            hours_scaled[int(hour)][0],\n",
    "        ]\n",
    "        vector_original = [\n",
    "            date,\n",
    "            hour,\n",
    "            hours_scaled[int(hour)][0],\n",
    "        ]\n",
    "        for currency in currencies:\n",
    "            d1 = scaled_data[currency]  \n",
    "            if d1.get(date, False) and not d1[date].get(hour, pd.Series()).empty:\n",
    "                vector += list(d1[date][hour])\n",
    "            else:\n",
    "                vector += [np.nan for i in range(0, len(columns_as_feature))]\n",
    "\n",
    "            d2 = original_data[currency]\n",
    "            if d2.get(date, False) and not d2[date].get(hour, pd.Series()).empty:\n",
    "                vector_original += list(d2[date][hour])\n",
    "            else:\n",
    "                vector_original += [np.nan for i in range(0, len(columns_as_feature))]\n",
    "\n",
    "        vectors.append(vector)\n",
    "        vectors_original.append(vector_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_shared_new = pd.DataFrame(vectors, columns=df_shared_columns)\n",
    "df_shared_new.to_csv('data/shared/transformed.csv', index=False)\n",
    "\n",
    "df_shared_original_new = pd.DataFrame(vectors_original, columns=df_shared_columns)\n",
    "df_shared_original_new.to_csv('data/shared/original.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "currency_to_predict = 'ZCL'\n",
    "features_to_use = columns_as_feature\n",
    "currencies_to_use = currencies\n",
    "\n",
    "model_to_use = PolynomialFeatures()\n",
    "use_logistic_regression = type(model_to_use) is LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_label(label):\n",
    "    return pd.DataFrame([label], columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_t = df_transformed[currency_to_predict]\n",
    "labels = pd.DataFrame(columns=['label'])\n",
    "\n",
    "for date in all_dates:\n",
    "    for hour in all_hours:\n",
    "        label = df_t.loc[(df_t['date'] == date) & (df_t['hour'] == hour + 1)]\n",
    "        \n",
    "        if label['price_close'].empty:\n",
    "            price_close = np.nan\n",
    "        else:\n",
    "            price_close = float(label['price_close'])\n",
    "        labels = labels.append(\n",
    "            build_label(\n",
    "                price_close,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_shared = load_data('data/shared/transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chop off last row since there is are no future change to know\n",
    "def reset_index(df):\n",
    "    return df.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "if use_logistic_regression:\n",
    "    rows_to_trim_start = 36 # Logistic regression\n",
    "    number_of_rows_to_trim_end = 1 # Logistic regression\n",
    "else:\n",
    "    rows_to_trim_start = 0\n",
    "    number_of_rows_to_trim_end = 0\n",
    "\n",
    "df_shared_final = reset_index(df_shared[rows_to_trim_start:row_count - number_of_rows_to_trim_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_count = df_shared.shape[0] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add labels\n",
    "if use_logistic_regression:\n",
    "    labels = pd.DataFrame(columns=['label'])\n",
    "    \n",
    "    for i in range(0, row_count - 1):\n",
    "        label_idx = len(shared_initial_columns) + (\n",
    "            (\n",
    "                currencies_to_use.index(currency_to_predict) * len(features_to_use)\n",
    "            ) + 2\n",
    "        )\n",
    "        label_now = df_shared.iloc[i][label_idx]\n",
    "        label_next_hour = df_shared.iloc[i + 1][label_idx]\n",
    "\n",
    "        if label_next_hour > label_now:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        labels = labels.append(build_label(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 374)"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_final = reset_index(labels[rows_to_trim_start:row_count])\n",
    "len(df_shared_final), len(labels_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def labels_to_array(df):\n",
    "    return list(df['label'])\n",
    "\n",
    "def training_and_test_sets(df):\n",
    "    # Split data\n",
    "    stratified_split = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=0.16,\n",
    "        random_state=42,\n",
    "    )\n",
    "    # for train_index, test_index in \n",
    "    gen = stratified_split.split(\n",
    "        df,\n",
    "        df[['date', 'hour']],\n",
    "    )\n",
    "    for training_indices, test_indices in gen:\n",
    "        training_set = df.loc[training_indices].drop(['date', 'hour'], axis=1)\n",
    "        training_set_labels = labels_final.loc[training_indices]\n",
    "\n",
    "        test_set = df.loc[test_indices].drop(['date', 'hour'], axis=1)\n",
    "        test_set_labels = labels_final.loc[test_indices]\n",
    "    \n",
    "    return (training_set, training_set_labels, test_set, test_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = ['hour_scaled'] + currency_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train(opts={}):\n",
    "    features_to_use = opts.get('features_to_use', all_features)\n",
    "    r_model = opts['model_to_use']\n",
    "    verbose = opts.get('verbose', False)\n",
    "    \n",
    "    training_set, training_set_labels, test_set, test_set_labels = training_and_test_sets(df_shared_final)\n",
    "    training_set_to_use = training_set[features_to_use]\n",
    "    test_set_to_use = test_set[features_to_use]\n",
    "    \n",
    "    model_name = type(model_to_use)\n",
    "    param_grid = {\n",
    "        LinearRegression: {  \n",
    "        },\n",
    "        LogisticRegression: {\n",
    "            # {'C': 10000.0, 'max_iter': 100, 'random_state': 0, 'solver': 'liblinear'}\n",
    "            # C_range = np.logspace(-2, 10, num=3)\n",
    "            # solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "            \n",
    "            'C': [10000],\n",
    "            'max_iter': [100],\n",
    "            'random_state': [0],\n",
    "            'solver': ['liblinear'],\n",
    "        },\n",
    "        PolynomialFeatures: {\n",
    "            'degree': [2 + i for i in range(1, 4)],\n",
    "            'include_bias': [True, False],\n",
    "            'interaction_only': [False, True],\n",
    "        },\n",
    "    }[model_name]\n",
    "    \n",
    "    scoring = {\n",
    "        LinearRegression: 'neg_mean_squared_error',\n",
    "        LogisticRegression: 'accuracy',\n",
    "        PolynomialFeatures: 'neg_mean_squared_error',\n",
    "    }[model_name]\n",
    "    \n",
    "    gs = GridSearchCV(\n",
    "        r_model,\n",
    "        param_grid,\n",
    "        cv=2,\n",
    "        scoring=scoring,\n",
    "        verbose=(10 if verbose else 0),\n",
    "    )\n",
    "    gs.fit(training_set_to_use, labels_to_array(training_set_labels))\n",
    "\n",
    "    if verbose:\n",
    "        print(gs.best_params_)\n",
    "        \n",
    "    gs.best_estimator_\n",
    "    gs.cv_results_\n",
    "    \n",
    "    model = gs.best_estimator_\n",
    "    score = model.score(test_set_to_use, labels_to_array(test_set_labels))\n",
    "\n",
    "    if verbose:\n",
    "        print('Score: {}'.format(score))\n",
    "\n",
    "    feature_importances = sorted(\n",
    "        zip(\n",
    "            model.coef_[0] if use_logistic_regression else model.coef_,\n",
    "            features_to_use,\n",
    "        ),\n",
    "        key=lambda x: np.absolute(x[0]),\n",
    "        reverse=True,\n",
    "    )\n",
    "    best_features = [f for s, f in feature_importances]\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of features: {}'.format(len(best_features)))\n",
    "        \n",
    "    return (\n",
    "        score,\n",
    "        best_features,\n",
    "        model,\n",
    "        feature_importances,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PolynomialFeatures' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-858-659cdd2d6b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     score, best_features_to_use, model, feature_importances = train({\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'features_to_use'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;34m'model_to_use'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_to_use\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     }) \n\u001b[1;32m     15\u001b[0m     \u001b[0mscores_and_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_features_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-857-68406faa9aa9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opts)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     89\u001b[0m         super(_PredictScorer, self).__call__(estimator, X, y_true,\n\u001b[1;32m     90\u001b[0m                                              sample_weight=sample_weight)\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PolynomialFeatures' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "scores_and_features = []\n",
    "best_features_to_use = None\n",
    "\n",
    "n_times_to_try = 200\n",
    "for i in range(0, n_times_to_try):\n",
    "    if best_features_to_use:\n",
    "        end_idx = max([n_times_to_try - i, 1])\n",
    "        arr = best_features_to_use[0:end_idx]\n",
    "    else:\n",
    "        arr = all_features\n",
    "    score, best_features_to_use, model, feature_importances = train({\n",
    "        'features_to_use': arr,\n",
    "        'model_to_use': model_to_use,\n",
    "    }) \n",
    "    scores_and_features.append((score, best_features_to_use, model, feature_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_scores = sorted(scores_and_features, key=lambda x: x[0], reverse=True)\n",
    "for tup in best_scores[0:3]:\n",
    "    print(tup[0], len(tup[1]))\n",
    "best_score = best_scores[0]\n",
    "best_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "model_name = '{}_{}'.format(str(type(model_to_use)).lower(), currency_to_predict.lower())\n",
    "\n",
    "joblib.dump(best_score[2], 'models/{}.pkl'.format(model_name))\n",
    "loaded_model = joblib.load('models/{}.pkl'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597.385757042\n",
      "0.49667175761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Test loaded data\n",
    "_, _, test_set, test_set_labels = training_and_test_sets(df_shared_final)\n",
    "if use_logistic_regression:\n",
    "    score = loaded_model.score(test_set[best_score[1]], labels_to_array(test_set_labels))\n",
    "else:\n",
    "    y = labels_to_array(test_set_labels)\n",
    "    y_predict = loaded_model.predict(test_set[best_score[1]])\n",
    "    score = mean_squared_error(y, y_predict)\n",
    "    percent_diff = [np.absolute(x) for x in (y - y_predict) / y]\n",
    "    print(sum(percent_diff) / len(percent_diff)) # Off by 12% on average\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(best_score[3], columns=['weight', 'feature']).to_csv(\n",
    "    'models/{}_features.csv'.format(model_name),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-933-280b2a13c807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_and_test_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_shared_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                                           self.include_bias)\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0mXP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mXP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# next_hour_price = loaded_model.predict(df_shared_final[best_score[1]])\n",
    "# df_shared_final['next_hour_price'] = StandardScaler().fit_transform(pd.DataFrame(next_hour_price))\n",
    "# df_shared_final\n",
    "\n",
    "training_set, training_set_labels, test_set, test_set_labels = training_and_test_sets(df_shared_final)\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_ = poly.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lg = LinearRegression()\n",
    "lg.fit(X_, labels_to_array(training_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_ = poly.fit_transform(test_set)\n",
    "predictions = lg.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(labels_to_array(test_set_labels), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labels_to_array(test_set_labels)\n",
    "percent_diff = [np.absolute(x) for x in (y - predictions) / y]\n",
    "print(sum(percent_diff) / len(percent_diff)) # Off by 12% on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_output_features = poly.n_output_features_\n",
    "feature_names = poly.get_feature_names()\n",
    "\n",
    "feature_importances = sorted(\n",
    "    zip(\n",
    "        lg.coef_,\n",
    "        feature_names,\n",
    "        [i for i in range(0, n_output_features)],\n",
    "    ),\n",
    "    key=lambda x: np.absolute(x[0]),\n",
    "    reverse=True,\n",
    ")\n",
    "best_features = [(f, i) for s, f, i in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_feature_names = [f for f, i in best_features[0:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set, training_set_labels, test_set, test_set_labels = training_and_test_sets(df_shared_final)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_ = poly.fit_transform(training_set)\n",
    "X_df = pd.DataFrame(X_, columns=feature_names)\n",
    "\n",
    "lg = LinearRegression()\n",
    "lg.fit(X_df[best_feature_names], labels_to_array(training_set_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_ = poly.fit_transform(test_set)\n",
    "X_test_df_ = pd.DataFrame(X_test_, columns=feature_names)\n",
    "predictions = lg.predict(X_test_df_[best_feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(labels_to_array(test_set_labels), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = labels_to_array(test_set_labels)\n",
    "percent_diff = [np.absolute(x) for x in (y - predictions) / y]\n",
    "print(sum(percent_diff) / len(percent_diff)) # Off by 12% on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
